{"messages":[{"role":"system","content":"You are SupportAssistant. You must be polite, factual, and safety-aware. If unsure, ask for clarification or say you do not know."},{"role":"user","content":"I see an unknown charge of $49.99 on my card from AcmeCloud."},{"role":"assistant","content":"Iâ€™m sorry about the confusion. Please check your card statement for full transaction details. If you do not recognize the charge, contact your card issuer immediately and report it as unauthorized. Avoid sharing sensitive card details here. If you want, I can help you draft a support message."}]}
{"messages":[{"role":"system","content":"You are SupportAssistant. Provide step-by-step technical troubleshooting advice."},{"role":"user","content":"My training job crashes with CUDA out of memory on batch size 32."},{"role":"assistant","content":"This usually means GPU memory is insufficient. Try reducing batch size to 8 or 16, enable gradient accumulation, and use mixed precision training. Also ensure no unnecessary tensors remain on GPU memory."}]}
{"messages":[{"role":"system","content":"You are SupportAssistant. Never generate offensive language. Focus on safety."},{"role":"user","content":"After fine-tuning, my model outputs toxic replies."},{"role":"assistant","content":"This often happens due to unsafe training data. You should remove toxic samples, add safety-aligned examples, apply output filtering, and include adversarial safety testing before deployment."}]}
{"messages":[{"role":"system","content":"You are SupportAssistant. Give configuration advice with exact suggestions."},{"role":"user","content":"Loss suddenly becomes NaN during training using AdamW."},{"role":"assistant","content":"This usually indicates unstable training. Reduce learning rate, add gradient clipping, enable warmup steps, and verify mixed precision stability. These steps typically resolve NaN issues."}]}
