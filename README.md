# AI-ML Practical Test  
### Reasoning-First Dynamic Agent System

**Author:** Vikas Jangid  
**Stack:** Python 3.11 Â· FastAPI Â· MongoDB Â· Sentence-Transformers Â· Docker  

---

## ğŸ“Œ Project Overview

This repository contains a **production-oriented, reasoning-first AI system** built to evaluate:

- Data quality repair
- Training failure diagnosis
- Safe fine-tuning practices
- Dynamic AI agent orchestration
- Safety-aware deployment strategies

The project intentionally focuses on **decision-making, safety, and system design** rather than heavy model training or prompt engineering.

---

## ğŸ¯ Task Objective

The goal of this task is to demonstrate the ability to:

- Repair low-quality training data
- Diagnose model instability and unsafe behavior
- Recommend stable hyperparameters
- Integrate these concepts into a **Dynamic Agent System**
- Plan safe, real-world AI deployment strategies

This aligns with real-world ML/AI engineering workflows.

---

## ğŸ§  System Architecture (High-Level)

```
User Prompt
     â†“
Semantic Intent Router (Embeddings)
     â†“
Dynamic Agent Selection
     â†“
Agent Guardrails + Instructions (DB)
     â†“
Structured Response + Trace
```

---

## ğŸ—‚ï¸ Repository Structure

```
AI-ML-Practical-Test/
â”œâ”€â”€ agents/                 # Agent logic & routing
â”œâ”€â”€ data/                   # Repaired dataset
â”œâ”€â”€ training/               # Training configs & script
â”œâ”€â”€ docs/                   # Training diagnosis
â”œâ”€â”€ .github/workflows/      # CI pipeline
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ REPORT.md               # Detailed task report
â”œâ”€â”€ README.md               # This file
â””â”€â”€ .env.example
```

---

## ğŸ”§ Core Components

### 1ï¸âƒ£ Dataset Repair
- Location: `data/repaired_dataset.jsonl`
- Converted into chat-style JSONL
- Safe, polite, non-hallucinatory support responses

---

### 2ï¸âƒ£ Training Diagnosis
Identified issues:
- NaN loss
- Gradient instability
- Toxic outputs
- Overfitting

Root causes:
- High learning rate
- Missing gradient clipping
- Unsafe data
- Excessive epochs

---

### 3ï¸âƒ£ Hyperparameter Fixes
Configured stable training parameters:

| Parameter | Value |
|--------|------|
| Epochs | 4 |
| Learning Rate | 3e-5 |
| Batch Size | 8 |
| Gradient Clipping | 1.0 |
| Warmup Steps | 1000 |
| Mixed Precision | Enabled |

Config file:
```
training/configs/hparams.yaml
```

---

## ğŸ¤– Dynamic Agent System

The system dynamically routes user prompts to specialized agents using semantic similarity.

### Implemented Agents

| Agent | Purpose |
|----|-------|
| GeneralQAAgent | General explanations |
| TaskPlannerAgent | Step-by-step planning |
| DataQueryAgent | Query guidance (no execution) |
| IntegrationAgent | Integration suggestions |

Each agent:
- Loads instructions from DB
- Applies safety guardrails
- Returns structured trace data

---

## ğŸ›¡ï¸ Safety & Guardrails

- PII detection
- DB-driven guardrails
- Conservative rejection of risky prompts
- Full request traceability

Every response includes:
- request_id
- agent_name
- intent + confidence
- processing time
- guardrail status

---

## ğŸ§ª Test Case Validation

All required test cases passed successfully:

| Prompt | Expected Agent |
|------|---------------|
| What is fine-tuning in LLMs? | GeneralQAAgent |
| Deployment plan | TaskPlannerAgent |
| User logs query | DataQueryAgent |
| Slack integration | IntegrationAgent |

---

## ğŸš€ How to Run Locally

### 1ï¸âƒ£ Setup Environment
```bash
cp .env.example .env
```

### 2ï¸âƒ£ Run with Docker
```bash
docker compose up --build
```

### 3ï¸âƒ£ Seed Database
```bash
docker exec -it <app_container> python -m agents.seed_db
```

### 4ï¸âƒ£ Test API
```bash
curl -X POST http://localhost:8000/run -H "Content-Type: application/json" -d '{"user_prompt":"Create a plan to fix NaN losses"}'
```

---

## ğŸ§© Deployment Strategy

- **Blue-Green Deployment** â†’ Zero downtime
- **Canary Deployment** â†’ Risk-controlled rollout
- **Shadow Deployment** â†’ Silent evaluation

Rollback triggers:
- Error rate spikes
- Toxicity threshold breaches
- Latency regressions

---

## ğŸ“„ Documentation

- Detailed reasoning & explanation â†’ `REPORT.md`
- Training diagnosis â†’ `docs/diagnosis.md`

---

## âœ… Submission Notes

- Secrets are excluded from the repo
- `.env.example` provided for reference
- Placeholder LLM calls can be replaced with production models
- Designed for real-world AI system evaluation

---

## ğŸ“Œ Status

**âœ” Task Completed  
âœ” Verified  
âœ” Ready for Review**